---
title: "Project Report"
author: "Mahesh Kumar"
date: "7/16/2017"
output: html_document
---

```{r setup0, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Background

The advent of wearable devices such as Jawbone, AppleWatch and others have enabled its wearers to estimate ‘how much’ exercise they are getting.  However, one unaddressed problem is determining ‘how well’ they are doing the exercise.   The goal of this project is to determine if its possible to ascertain how well users are performing exercise.

Support for this task was provided through the generous contribution and through the data generated by the authors of the following research report:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz4n149Y2mr

##Observations on Data Collection
From the report above:
“Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience.”

1. Data was collected on participants of similar age group as a result age might not be a factor, even if its missing from the data it may not affect the outcome when we predict in a general population of a similar age group.
2. All participants had similar experience level (low) with weight training, thereby ensuring that the experience level will not skew the observations.
3. A good spread of outcomes, including four common mistakes and the correct technique to do exercise were chosen, thus covering the range of possible outcomes in a general population.
4. Number of observations for each possible outcome is balanced.  For example: data from a training sample show the following numbers  for possible outcomes- Classe A: 4185, Classe B: 2848, Classe C: 2567, Classe D: 2412, Classe E: 2707.   
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```
##Process Steps
###1. Load the Data
```{r pressure, echo=TRUE, message=FALSE}
library(caret)
library(rpart)
library(randomForest)
excercise = read.csv("~/Downloads/pml-training.csv")
```

###2. Split data into Training and Validation sets
```{r setup1, echo=TRUE, message=FALSE}
inTrain = createDataPartition(excercise$classe,p=3/4)[[1]]
training = excercise[inTrain,]
testing = excercise[-inTrain,]
```

###3. Review Training Data for the following:
1. Identify variables to reduce model size (near zero vars).  Use the nearZeroVar function to identify variables not useful in prediction. 
2. Detect anomalies in the data (missing data etc.).  Examples marked by orange ovals in the diagram below. 
3. Identify data that should not be in the prediction (e.g. Serial Number etc.).  Examples are marked by a a red oval in diagram.

```{r setup2, echo=TRUE, message=FALSE}
nzv <- nearZeroVar(training)
training1 <- training[,-nzv]
testing1 <- testing[,-nzv]
dim(training1)
dim(testing1)
trainNew <- training1[,colSums(is.na(training1))==0]
trainNewFinal <- trainNew[,-c(1:5)]
testNew <- testing1[,colSums(is.na(testing1))==0]
testNewFinal <- testNew[,-c(1:5)]
```
##Model Selection
```{r setup3, echo=TRUE, message=FALSE}
modRf <- train(classe ~., method="rf", data=trainNewFinal)
modRf$finalModel
```
The above model shows that OOB (out-of-bag) error rate is 0.29%, which is very low.  So the model we selected is expected to perform well in test data.  I ran the model in the validation data (testNewFinal) and the model produced 100% correct results.  Since the OOB error is very low and the validation data provided good results I decided not to test with other models.

##Model Validation
```{r setup4, echo=TRUE, message=FALSE}
predictRf <- predict(modRf, newdata=testNewFinal)
confusionMatrix(predictRf, testNewFinal$classe)
```
The results show very high level of accuracy of predictions, which is consistent with the model estimated the OOB error.

##Results
Finally, I tested the model using the testing data provided.  This data had no part in any model formation/definition. 

```{r setup5, echo=TRUE, message=FALSE}

excerciseTest = read.csv("~/Downloads/pml-testing.csv")
```
The model was then used to predict the test data set that was in the test csv file.  Here are the results of that prediction. 
```{r setup6, echo=TRUE, message=FALSE}
predRfA <- predict(modRf, excerciseTest)
print(predRfA)
```
The above results were input in the class project quiz and the answers were 100% correct.